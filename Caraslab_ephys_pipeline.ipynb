{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Caras lab ephys analysis pipeline\n",
    "This pipeline is intended to be run after extracting behavioral timestamps and neuron spike times with our [MatLab pipeline](https://github.com/caraslab/caraslab-spikesortingKS2)\n",
    "\n",
    "Files need to be organized in a specific folder structure or file paths need to be changed\n",
    "\n",
    "File structure can be found in the Sample dataset folder"
   ],
   "id": "344dc9b19a3e6c21"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Imports\n",
    "Specific imports can be found within each function"
   ],
   "id": "8adc8165366abc8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T00:12:50.519561Z",
     "start_time": "2025-03-28T00:12:47.708600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from os import remove, makedirs\n",
    "import warnings\n",
    "from os.path import sep\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from glob import glob\n",
    "\n",
    "from helpers.run_ephys_pipeline import run_pipeline\n",
    "from helpers.compile_fr_result_csv import compile_fr_result_csv\n",
    "from matplotlib.pyplot import rcParams\n",
    "from helpers.get_JSON_data import get_JSON_data\n",
    "from helpers.PSTH_plotter_fromJSON import run_PSTH_pipeline\n",
    "from auROC_analysis.auROC_heatmap_plotter import run_auROC_heatmap_pipeline\n",
    "from helpers.recalculate_ePsych_responseLatency import recalculate_ePsych_responseLatency\n",
    "from auROC_analysis.auROC_tslearnClustering_mp import run_ts_clustering\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Some plotting parameters\n",
    "label_font_size = 11\n",
    "tick_label_size = 7\n",
    "legend_font_size = 6\n",
    "line_thickness = 1\n",
    "\n",
    "rcParams['figure.dpi'] = 600\n",
    "rcParams['pdf.fonttype'] = 42\n",
    "rcParams['ps.fonttype'] = 42\n",
    "rcParams['font.family'] = 'Arial'\n",
    "rcParams['font.weight'] = 'regular'\n",
    "rcParams['axes.labelweight'] = 'regular'\n",
    "\n",
    "rcParams['font.size'] = label_font_size\n",
    "rcParams['axes.labelsize'] = label_font_size\n",
    "rcParams['axes.titlesize'] = label_font_size\n",
    "rcParams['axes.linewidth'] = line_thickness\n",
    "rcParams['legend.fontsize'] = legend_font_size\n",
    "rcParams['xtick.labelsize'] = tick_label_size\n",
    "rcParams['ytick.labelsize'] = tick_label_size\n",
    "rcParams['errorbar.capsize'] = label_font_size\n",
    "rcParams['lines.markersize'] = line_thickness\n",
    "rcParams['lines.linewidth'] = line_thickness"
   ],
   "id": "30ef205fa68127e6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Set global paths and variables\n",
    "\n",
    "FILE NAMING REQUIREMENTS\n",
    "\n",
    "This pipeline matches files using filenames.\n",
    "\n",
    "If you make changes to the filenaming structures you will have to edit the filename matching code\n",
    "\n",
    "Here are the defaults that come out of the MatLab processing pipeline:\n",
    "- Synapse:\n",
    "    - Behavior file: SUBJ-ID-154_MML-Aversive-AM-210501-112033_trialInfo.csv\n",
    "    - Spike time file: SUBJ-ID-154_210501_concat_cluster2627.txt\n",
    "- Intan:\n",
    "    - Behavior file: SUBJ-ID-231_2021-07-17_15-19-28_Active_trialInfo.csv\n",
    "    - Spike time file: SUBJ-ID-231_210717_concat_cluster651.txt\n",
    "\n",
    "If you need to alter the filename matching structure, edit this function: helpers.preprocess_files.find_spoutfile_and_breakpoint"
   ],
   "id": "205fcc63f736d166"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ea5b5cb7628864c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T00:13:08.977278Z",
     "start_time": "2025-03-28T00:13:08.864259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_PATH = '.' + sep + 'Sample_data'\n",
    "\n",
    "SETTINGS_DICT = {\n",
    "    'EXPERIMENT_TAG': 'OFCPL',  # Appends to start of summary files\n",
    "    'SPIKES_PATH': DATA_PATH + sep + 'Spike times',\n",
    "    'KEYS_PATH': DATA_PATH + sep + 'Key files',\n",
    "    'OUTPUT_PATH': DATA_PATH + sep + 'Output',\n",
    "    \n",
    "    # If your data was concatenated before Kilosort\n",
    "    'BREAKPOINT_PATH': DATA_PATH + sep + 'Breakpoints',\n",
    "    \n",
    "    # in seconds; for firing rate calculation to non-AM trials\n",
    "    'NONAM_DURATION_FOR_FR': 0.9,\n",
    "    \n",
    "    # in seconds; for firing rate calculation to AM trials; shock artifact starts at ~0.9\n",
    "    'TRIAL_DURATION_FOR_FR': {'Hit': 0.9, 'FA': 0.9, 'Miss': 0.9, 'Passive': 0.9},\n",
    "    \n",
    "    'AFTERTRIAL_FR_START': {'Hit': 1, 'FA': 1, 'Miss': 1.3, 'Passive': 1},  # Shock artifact is ~0.3s long\n",
    "    'AFTERTRIAL_FR_END': {'Hit': 1.9, 'FA': 1.9, 'Miss': 2.2, 'Passive': 1.9},  # 0.9 duration to keep window consistent with trial\n",
    "    \n",
    "    'RESPTIME_FR_START': {'Hit': 0, 'FA': 0, 'Miss': 0.3, 'Passive': 0},  # Start of window following response. Irrelevant for passive\n",
    "    'RESPTIME_FR_END': {'Hit': 1.9, 'FA': 1.9, 'Miss': 2.25, 'Passive': 1.9},  # 0.9 duration to keep window consistent with trial\n",
    "    \n",
    "    'BEFORERESP_FR_START': {'Hit': 0.5, 'FA': 0.5, 'Miss': 0.8, 'Passive': 0.5},  # Start of window preceding response. Irrelevant for passive\n",
    "    'BEFORERESP_FR_END': {'Hit': 0, 'FA': 0, 'Miss': 0.3, 'Passive': 0},  # End of window preceding response\n",
    "    \n",
    "    # Unused for now but keep this period in mind when analyzing shocked misses\n",
    "    'SHOCK_START_END': [0.95, 1.3],\n",
    "    \n",
    "    # These are for raw data extraction (no FR calculations)\n",
    "    'PRETRIAL_DURATION_FOR_SPIKETIMES': 2,  # in seconds; for grabbing spiketimes around AM trials\n",
    "    'POSTTRIAL_DURATION_FOR_SPIKETIMES': 5,  # in seconds; for grabbing spiketimes around AM trials\n",
    "    \n",
    "    'MULTIPROCESS': True,  # Turn off multiprocessing for easier debugging\n",
    "    \n",
    "    # For multiprocessing. Defaults to 4/5s of the number of cores\n",
    "    'NUMBER_OF_CORES': 4 * cpu_count() // 5,\n",
    "\n",
    "    # Only run these cells/subjects/sessions or None to run all\n",
    "    'SESSIONS_TO_RUN': None,  # You can specify parts of the file name too\n",
    "    'SESSIONS_TO_EXCLUDE': None,\n",
    "    \n",
    "    'OVERWRITE_PREVIOUS_CSV': True,  # False: appends to existing firing rate CSV file\n",
    "    \n",
    "    # Set up your recording platforms here\n",
    "    'RECORDING_TYPE_DICT': {\n",
    "        'SUBJ-ID-197': 'synapse',\n",
    "        'SUBJ-ID-151': 'synapse',\n",
    "        'SUBJ-ID-154': 'synapse',\n",
    "        'SUBJ-ID-231': 'intan',\n",
    "        # 'SUBJ-ID-232': 'intan',\n",
    "        # 'SUBJ-ID-270': 'intan',\n",
    "        'SUBJ-ID-389': 'intan',\n",
    "        'SUBJ-ID-390': 'intan'\n",
    "    },\n",
    "\n",
    "    # Only needed for some older recordings processed by the MatLab pipeline. Newer files contain the sampling rates in them\n",
    "    'SAMPLING_RATE_DICT': {\n",
    "        'synapse': 24414.0625,\n",
    "        'intan': 30000\n",
    "    },\n",
    "    \n",
    "    # PSTH generation settings\n",
    "    'PSTH_BIN_SIZE': 0.01,\n",
    "    'PSTH_PRE_STIMULUS_DURATION': 2,\n",
    "    'PSTH_POST_STIMULUS_DURATION': 4,\n",
    "    'PSTH_FIXED_YLIM': 150,\n",
    "    'PSTH_RASTER_YLIM': 30.5,  # ~Number of trials to display in the raster plot\n",
    "    'PSTH_ALIGN_TO_RESPONSE': False,\n",
    "    'PSTH_TRIALTYPES': [\n",
    "        'Hit (shock)',  # Trials above threshold\n",
    "        # 'Hit (no shock)', # Trials below threshold\n",
    "        'False alarm',\n",
    "        'Miss (shock)',  # Trials above threshold\n",
    "        # 'Miss (no shock)', # Trials below threshold\n",
    "        'Passive',\n",
    "    ],\n",
    "    \n",
    "    #########################################\n",
    "    # AuROC heatmap generation settings\n",
    "    \n",
    "    # Implemented auROC analyses types:\n",
    "    # Trial-aligned types:\n",
    "    #   TrialAligned_PassivePre_auroc, TrialAligned_Hit_shockFlagOn_auroc, TrialAligned_FA_auroc, \n",
    "    #   TrialAligned_Miss_shockFlagOn_auroc, TrialAligned_PassivePost_auroc, TrialAligned_GO_auroc, TrialAligned_GO_byAMdepth_auroc\n",
    "    # Response-aligned types:\n",
    "    #   ResponseAligned_Hit_auroc, ResponseAligned_Hit_shockFlagOn_auroc, ResponseAligned_FA_auroc, \n",
    "    #   ResponseAligned_Miss_shockOn_auroc, ResponseAligned_Hit_byAMdepth_auroc, ResponseAligned_Miss_shockOn_byAMdepth_auroc\n",
    "    \n",
    "    # If you want to implement a new one, simply tweak run_ephys_pipeline.py\n",
    "    #########################################\n",
    "    \n",
    "    # This grouping file can contain any number of characteristics, such as cell type, response characteristics, clustering IDs etc. Has to be generated elsewhere; Otherwise set to None\n",
    "    'AUROC_GROUPING_FILE': None, # DATA_PATH + sep + 'allUnits_list.csv',  # Or None\n",
    "    'AUROC_GROUPING_VARIABLE': 'ActiveBaseline_modulation_direction',  # Or None\n",
    "    'AUROC_UNIQUE_GROUPS': ['decrease', 'increase', 'none'],\n",
    "    'AUROC_GROUP_COLORS': ['#E49E50', '#5AB4E5', '#939598'],\n",
    "    \n",
    "    'AUROC_TRIALTYPES': {\n",
    "        # Use these if you would like to sort based on trial (AM)-aligned auROC\n",
    "        'TrialAligned_PassivePre_auroc': [0, 1.5],\n",
    "        'TrialAligned_Hit_shockFlagOn_auroc': [0, 1.5],\n",
    "        'TrialAligned_FA_auroc': [0, 1.5],\n",
    "        'TrialAligned_Miss_shockFlagOn_auroc': [1.4, 2.9],\n",
    "        'TrialAligned_PassivePost_auroc': [0, 1.5],\n",
    "        \n",
    "        # Use these if you would like to sort based on response (spout offset)-aligned auROC\n",
    "        # 'ResponseAligned_Hit_auroc': [-0.5, 0],\n",
    "        # 'ResponseAligned_FA_auroc': [-0.5, 0],\n",
    "        # 'ResponseAligned_Miss_shockOn_auroc': [-0.5, 0]\n",
    "    },\n",
    "    \n",
    "\n",
    "    'SORT_BY_WHICH_TRIALTYPE': 'TrialAligned_Hit_shockFlagOn_auroc',\n",
    "    \n",
    "    'AUROC_BIN_SIZE': 0.1,\n",
    "    'AUROC_PRE_STIMULUS_DURATION': 2,\n",
    "    \n",
    "    'AUROC_POST_STIMULUS_DURATION': 4,\n",
    "    \n",
    "    #########################################\n",
    "    # Time series clustering parameters\n",
    "    #########################################\n",
    "    'MAXCLUSTERS': 10,  # maximum number of clusters to be evaluated (10)\n",
    "    'BOOT_N': 50,  # number of iterations (maximum of 50 recommended due to computing time for DTW) (50)\n",
    "    'SK_FACTOR': 2,  # error multiplication factor for choosing best number of clusters (2)\n",
    "    'USE_GPU': False,  # If true, overrides the global multiprocessing flag; Not implemented yet\n",
    "    'USE_TIBSHIRANI_CRITERION': True,  # If true, use f(k) ≥ f(k+1) - s{k+1}; if false, use the stricter f(k) + s{k} ≥ f(k+1) - s{k+1}\n",
    "    'TS_TRIALTYPES': {\n",
    "        # Use these if you would like to cluster based on trial (AM)-aligned auROC\n",
    "        # 'TrialAligned_PassivePre_auroc': [0, 1.5],\n",
    "        # 'TrialAligned_Hit_shockFlagOn_auroc': [0, 1.5],\n",
    "        # 'TrialAligned_FA_auroc': [0, 1.5],\n",
    "        # 'TrialAligned_Miss_shockFlagOn_auroc': [1.4, 2.9],\n",
    "        # 'TrialAligned_PassivePost_auroc': [0, 1.5],\n",
    "        \n",
    "        # Use these if you would like to cluster based on response (spout offset)-aligned auROC\n",
    "        'ResponseAligned_Hit_auroc': [0, 1.5],\n",
    "        'ResponseAligned_FA_auroc': [0, 1.5],\n",
    "        'ResponseAligned_Miss_shockFlagOn_auroc': [0.3, 1.8]\n",
    "    },\n",
    "    \n",
    "    #########################################\n",
    "    # Switchboard \n",
    "    # Below is a switchboard of functions you desire to run from the pipeline\n",
    "    # If you change your mind later, you can just run the ones you want and the code will add it to existing JSON files\n",
    "    #########################################\n",
    "    'PIPELINE_SWITCHBOARD': {\n",
    "        # Only relevant to AversiveAM task when recorded using older RPvds circuit\n",
    "        # ATTENTION!! THIS FUNCTION WILL OVERWRITE THE RespLatency COLUMN IN YOUR EPSYCH FILES\n",
    "        'recalculate_ePsych_responseLatency': True,\n",
    "        \n",
    "        # Trial-by-trial firing rates\n",
    "        'firing_rate_to_trials': True,  \n",
    "        \n",
    "        # auROC analyses need by definition to combine trials, so each of these pool a specific type of trial and align to specific events\n",
    "        # Trials aligned by AM onset\n",
    "        'TrialAligned_Hit_auroc': True,\n",
    "        'TrialAligned_Hit_shockFlagOn_auroc': True,\n",
    "\n",
    "        'TrialAligned_FA_auroc': True,\n",
    "        \n",
    "        'TrialAligned_Miss_auroc': True,\n",
    "        'TrialAligned_Miss_shockFlagOn_auroc': True,\n",
    "        \n",
    "        'TrialAligned_GO_auroc': True,  # Agnostic of whether miss or hit\n",
    "        'TrialAligned_GO_byAMdepth_auroc': True,  # Agnostic of whether miss or hit\n",
    "        \n",
    "        # Response-aligned (by spoutOff events)\n",
    "        'ResponseAligned_Hit_auroc': True,\n",
    "        'ResponseAligned_Hit_shockFlagOn_auroc': True,\n",
    "\n",
    "        'ResponseAligned_FA_auroc': True,\n",
    "        \n",
    "        'ResponseAligned_Miss_shockFlagOn_auroc': True,\n",
    "        \n",
    "        'ResponseAligned_Hit_byAMdepth_auroc': True,\n",
    "        'ResponseAligned_Miss_shockFlagOn_byAMdepth_auroc': True,\n",
    "\n",
    "        # PSTH plotting\n",
    "        'plot_trialType_PSTH': True,\n",
    "        'plot_AMDepth_PSTH': True\n",
    "    }\n",
    "    #########################################\n",
    "}\n"
   ],
   "id": "5f703569dd7af350",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Calculate response latency using spoutInfo files\n",
    "Only run this if your files come from an old (~2023 and earlier) RPvds circuit and did not record response latencies \n"
   ],
   "id": "731e823a8fb4dcf8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T00:00:10.412228Z",
     "start_time": "2025-03-28T00:00:08.473852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Before processing each unit's files, check if user wants to recalculate response latencies\n",
    "keys_paths = glob(SETTINGS_DICT['KEYS_PATH'] + sep + '*trialInfo.csv')\n",
    "if SETTINGS_DICT['PIPELINE_SWITCHBOARD']['recalculate_ePsych_responseLatency']:\n",
    "    recalculate_ePsych_responseLatency((keys_paths, SETTINGS_DICT))"
   ],
   "id": "7bed86f7c4752178",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spout offset not registered properly in: .\\Sample_data\\Key files\\SUBJ-ID-154_MML-Aversive-AM-210501-112033_trialInfo.csv\n",
      "TrialID: 239\n",
      "\n",
      "\n",
      "Spout offset not registered properly in: .\\Sample_data\\Key files\\SUBJ-ID-154_MML-Aversive-AM-210509-151840_trialInfo.csv\n",
      "TrialID: 171\n",
      "\n",
      "\n",
      "Spout offset not registered properly in: .\\Sample_data\\Key files\\SUBJ-ID-154_MML-Aversive-AM-210510-113653_trialInfo.csv\n",
      "TrialID: 209\n",
      "\n",
      "\n",
      "Spout offset not registered properly in: .\\Sample_data\\Key files\\SUBJ-ID-154_MML-Aversive-AM-210512-154912_trialInfo.csv\n",
      "TrialID: 70\n",
      "\n",
      "\n",
      "Spout offset not registered properly in: .\\Sample_data\\Key files\\SUBJ-ID-154_MML-Aversive-AM-210512-154912_trialInfo.csv\n",
      "TrialID: 254\n",
      "\n",
      "\n",
      "Spout offset not registered properly in: .\\Sample_data\\Key files\\SUBJ-ID-154_MML-Aversive-AM-210514-145555_trialInfo.csv\n",
      "TrialID: 14\n",
      "\n",
      "\n",
      "Spout offset not registered properly in: .\\Sample_data\\Key files\\SUBJ-ID-154_MML-Aversive-AM-210519-104756_trialInfo.csv\n",
      "TrialID: 14\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Initial file matching then run the pipeline\n",
    "Uses multiprocessing to process many units at once"
   ],
   "id": "818f159611f2b511"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T00:25:54.499073Z",
     "start_time": "2025-03-28T00:23:46.946954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "makedirs(SETTINGS_DICT['OUTPUT_PATH'] + sep + 'JSON files', exist_ok=True)\n",
    "\n",
    "# Load existing JSONs; will be empty if this is the first time running\n",
    "json_filenames = glob(SETTINGS_DICT['OUTPUT_PATH'] + sep + 'JSON files' + sep + '*json')\n",
    "\n",
    "# Clear older temp files if they exist\n",
    "process_tempfiles = glob(SETTINGS_DICT['OUTPUT_PATH'] + sep + '*_tempfile_*.csv')\n",
    "[remove(f) for f in process_tempfiles]\n",
    "\n",
    "# Generate a list of inputs to be passed to each worker\n",
    "input_lists = list()\n",
    "memory_paths = glob(SETTINGS_DICT['SPIKES_PATH'] + sep + '*cluster*.txt')\n",
    "keys_paths = glob(SETTINGS_DICT['KEYS_PATH'] + sep + '*trialInfo.csv')\n",
    "\n",
    "for dummy_idx, memory_path in enumerate(memory_paths):\n",
    "    if SETTINGS_DICT['SESSIONS_TO_RUN'] is not None:\n",
    "        if any([chosen for chosen in SETTINGS_DICT['SESSIONS_TO_RUN'] if chosen in memory_path]):\n",
    "            pass\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    if SETTINGS_DICT['SESSIONS_TO_EXCLUDE'] is not None:\n",
    "        if any([chosen for chosen in SETTINGS_DICT['SESSIONS_TO_EXCLUDE'] if chosen in memory_path]):\n",
    "            continue\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    if SETTINGS_DICT['MULTIPROCESS']:\n",
    "        input_lists.append((memory_path, json_filenames, SETTINGS_DICT))\n",
    "    else:\n",
    "        run_pipeline((memory_path, json_filenames, SETTINGS_DICT))\n",
    "\n",
    "if SETTINGS_DICT['MULTIPROCESS']:\n",
    "    pool = Pool(SETTINGS_DICT['NUMBER_OF_CORES'])\n",
    "\n",
    "    # # Feed each worker with all memory paths from one unit\n",
    "    pool_map_result = pool.map(run_pipeline, input_lists)\n",
    "\n",
    "    pool.close()\n",
    "\n",
    "    pool.join()\n",
    "    \n",
    "compile_fr_result_csv(SETTINGS_DICT['EXPERIMENT_TAG'], SETTINGS_DICT['OUTPUT_PATH'], SETTINGS_DICT['OVERWRITE_PREVIOUS_CSV'])"
   ],
   "id": "c7c54c53d89bee96",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 37\u001B[0m\n\u001B[0;32m     34\u001B[0m pool \u001B[38;5;241m=\u001B[39m Pool(SETTINGS_DICT[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNUMBER_OF_CORES\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     36\u001B[0m \u001B[38;5;66;03m# # Feed each worker with all memory paths from one unit\u001B[39;00m\n\u001B[1;32m---> 37\u001B[0m pool_map_result \u001B[38;5;241m=\u001B[39m \u001B[43mpool\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrun_pipeline\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_lists\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     39\u001B[0m pool\u001B[38;5;241m.\u001B[39mclose()\n\u001B[0;32m     41\u001B[0m pool\u001B[38;5;241m.\u001B[39mjoin()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\pool.py:367\u001B[0m, in \u001B[0;36mPool.map\u001B[1;34m(self, func, iterable, chunksize)\u001B[0m\n\u001B[0;32m    362\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmap\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, iterable, chunksize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    363\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[0;32m    364\u001B[0m \u001B[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001B[39;00m\n\u001B[0;32m    365\u001B[0m \u001B[38;5;124;03m    in a list that is returned.\u001B[39;00m\n\u001B[0;32m    366\u001B[0m \u001B[38;5;124;03m    '''\u001B[39;00m\n\u001B[1;32m--> 367\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_async\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapstar\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\pool.py:768\u001B[0m, in \u001B[0;36mApplyResult.get\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    767\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m--> 768\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    769\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mready():\n\u001B[0;32m    770\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\pool.py:765\u001B[0m, in \u001B[0;36mApplyResult.wait\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    764\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwait\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m--> 765\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_event\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:634\u001B[0m, in \u001B[0;36mEvent.wait\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    632\u001B[0m signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flag\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m signaled:\n\u001B[1;32m--> 634\u001B[0m     signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cond\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    635\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m signaled\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:334\u001B[0m, in \u001B[0;36mCondition.wait\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    332\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:    \u001B[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[0;32m    333\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 334\u001B[0m         \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    335\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    336\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Now we can use the JSONs exported from the code above for a faster exploration of the data",
   "id": "deea365829bb96c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T03:16:25.208630Z",
     "start_time": "2025-03-26T03:16:22.001315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sessions you want to run; edit the specifics in helpers.get_JSON_data if you want to change\n",
    "session_tags = ['pre', 'active', 'post', 'post1h']\n",
    "\n",
    "# Load existing JSONs\n",
    "json_filenames = glob(SETTINGS_DICT['OUTPUT_PATH'] + sep + 'JSON files' + sep + '*json')\n",
    "\n",
    "# Retrieve data in JSON\n",
    "data_dict = dict()\n",
    "print(\"Loading data in JSONs...\")\n",
    "for session_type in session_tags:\n",
    "    unit_list, data_list = (\n",
    "        get_JSON_data(json_filenames, session_type, \n",
    "                        sessions_to_run=SETTINGS_DICT['SESSIONS_TO_RUN'], \n",
    "                        sessions_to_exclude=SETTINGS_DICT['SESSIONS_TO_EXCLUDE'])\n",
    "    )\n",
    "    \n",
    "    for unit_idx, unit in enumerate(unit_list):\n",
    "        if unit not in data_dict.keys():\n",
    "            data_dict.update({unit: {}})\n",
    "        data_dict[unit].update({session_type: data_list[unit_idx]})"
   ],
   "id": "5fc8863493437641",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data in JSONs...\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PSTH plotting\n",
    "Unsophisticated PSTH plotting engine. Need to make tweaks if you want something fancier\n",
    "\n",
    "Uses multiprocessing to process many units at once"
   ],
   "id": "3f0dec45a485d216"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T03:16:46.829225Z",
     "start_time": "2025-03-26T03:16:26.539981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if SETTINGS_DICT['MULTIPROCESS']:\n",
    "    pool = Pool(SETTINGS_DICT['NUMBER_OF_CORES'])\n",
    "\n",
    "    # Feed each worker one unit_name\n",
    "    input_list = [(unit_name, data_dict[unit_name], SETTINGS_DICT) for unit_name in data_dict.keys()]\n",
    "    pool_map_result = pool.map(run_PSTH_pipeline, input_list)\n",
    "\n",
    "    pool.close()\n",
    "\n",
    "    pool.join()\n",
    "else:\n",
    "    [run_PSTH_pipeline((unit_name, data_dict[unit_name], SETTINGS_DICT)) for unit_name in data_dict.keys()]"
   ],
   "id": "8042d360245ce834",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# AuROC heatmaps\n",
    "Quick summary of what auROC profiles look like across the entire population."
   ],
   "id": "3801fee70507f18"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T03:17:34.751339Z",
     "start_time": "2025-03-26T03:17:34.365471Z"
    }
   },
   "cell_type": "code",
   "source": "run_auROC_heatmap_pipeline(data_dict, SETTINGS_DICT)",
   "id": "6d717d51aa97e18c",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Time series clustering\n",
    "This function calculates the optimal number of clusters from time series\n",
    "\n",
    "It is optimal for time series because it uses dynamic time warping (DTW) as a distance metric\n",
    "\n",
    "The optimal number of clusters is derived from an implementation of the gap-statistic by Tibshirani et al., 2001\n",
    "\n",
    "Results are output as csv/pdf files\n",
    "\n",
    "TODO: Add data to pre-existing JSON files maybe?\n"
   ],
   "id": "5b877529678ec4ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T03:18:43.171734Z",
     "start_time": "2025-03-26T03:17:39.516423Z"
    }
   },
   "cell_type": "code",
   "source": "run_ts_clustering(data_dict, SETTINGS_DICT)",
   "id": "8bd77d1d93171632",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running gap-stat clustering for metric ResponseAligned_Hit_auroc with 10 maxclusters and 50 simulations\n",
      "Loading data in JSONs...\n",
      "OptimalK start...\n",
      "Clustering data using 1 cluster(s)...\n",
      "Processing time: 0 min, 1.306 sec\n",
      "Clustering data using 2 cluster(s)...\n",
      "Processing time: 0 min, 0.020 sec\n",
      "Clustering data using 3 cluster(s)...\n",
      "Processing time: 0 min, 0.018 sec\n",
      "Clustering data using 4 cluster(s)...\n",
      "Processing time: 0 min, 0.026 sec\n",
      "Clustering data using 5 cluster(s)...\n",
      "Processing time: 0 min, 0.025 sec\n",
      "Clustering data using 6 cluster(s)...\n",
      "Processing time: 0 min, 0.037 sec\n",
      "Clustering data using 7 cluster(s)...\n",
      "Processing time: 0 min, 0.031 sec\n",
      "Clustering data using 8 cluster(s)...\n",
      "Processing time: 0 min, 0.030 sec\n",
      "Clustering data using 9 cluster(s)...\n",
      "Processing time: 0 min, 0.229 sec\n",
      "Clustering data using 10 cluster(s)...\n",
      "Processing time: 0 min, 0.232 sec\n",
      "Initializing 12 process(es) for clustering random distributions in 50 simulations x 10 clusters = 500 iterations\n",
      "Processing time: 0 min, 19.428 sec\n",
      "Compiling data and computing gap-statistic...\n",
      "Processing time: 0 min, 0.106 sec\n",
      "Gap-statistic found 1 clusters in data\n",
      "Total runtime: 0 min, 24.064 sec\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Running gap-stat clustering for metric ResponseAligned_FA_auroc with 10 maxclusters and 50 simulations\n",
      "Loading data in JSONs...\n",
      "OptimalK start...\n",
      "Clustering data using 1 cluster(s)...\n",
      "Processing time: 0 min, 0.017 sec\n",
      "Clustering data using 2 cluster(s)...\n",
      "Processing time: 0 min, 0.028 sec\n",
      "Clustering data using 3 cluster(s)...\n",
      "Processing time: 0 min, 0.023 sec\n",
      "Clustering data using 4 cluster(s)...\n",
      "Processing time: 0 min, 0.033 sec\n",
      "Clustering data using 5 cluster(s)...\n",
      "Processing time: 0 min, 0.026 sec\n",
      "Clustering data using 6 cluster(s)...\n",
      "Processing time: 0 min, 0.028 sec\n",
      "Clustering data using 7 cluster(s)...\n",
      "Processing time: 0 min, 0.034 sec\n",
      "Clustering data using 8 cluster(s)...\n",
      "Processing time: 0 min, 0.041 sec\n",
      "Clustering data using 9 cluster(s)...\n",
      "Processing time: 0 min, 0.336 sec\n",
      "Clustering data using 10 cluster(s)...\n",
      "Processing time: 0 min, 0.407 sec\n",
      "Initializing 12 process(es) for clustering random distributions in 50 simulations x 10 clusters = 500 iterations\n",
      "Processing time: 0 min, 16.567 sec\n",
      "Compiling data and computing gap-statistic...\n",
      "Processing time: 0 min, 0.148 sec\n",
      "Gap-statistic found 2 clusters in data\n",
      "Total runtime: 0 min, 20.531 sec\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Running gap-stat clustering for metric ResponseAligned_Miss_shockFlagOn_auroc with 10 maxclusters and 50 simulations\n",
      "Loading data in JSONs...\n",
      "OptimalK start...\n",
      "Clustering data using 1 cluster(s)...\n",
      "Processing time: 0 min, 0.015 sec\n",
      "Clustering data using 2 cluster(s)...\n",
      "Processing time: 0 min, 0.024 sec\n",
      "Clustering data using 3 cluster(s)...\n",
      "Processing time: 0 min, 0.021 sec\n",
      "Clustering data using 4 cluster(s)...\n",
      "Processing time: 0 min, 0.026 sec\n",
      "Clustering data using 5 cluster(s)...\n",
      "Processing time: 0 min, 0.027 sec\n",
      "Clustering data using 6 cluster(s)...\n",
      "Processing time: 0 min, 0.030 sec\n",
      "Clustering data using 7 cluster(s)...\n",
      "Processing time: 0 min, 0.034 sec\n",
      "Clustering data using 8 cluster(s)...\n",
      "Processing time: 0 min, 0.034 sec\n",
      "Clustering data using 9 cluster(s)...\n",
      "Processing time: 0 min, 0.229 sec\n",
      "Clustering data using 10 cluster(s)...\n",
      "Processing time: 0 min, 0.256 sec\n",
      "Initializing 12 process(es) for clustering random distributions in 50 simulations x 10 clusters = 500 iterations\n",
      "Processing time: 0 min, 15.238 sec\n",
      "Compiling data and computing gap-statistic...\n",
      "Processing time: 0 min, 0.144 sec\n",
      "Gap-statistic found 2 clusters in data\n",
      "Total runtime: 0 min, 18.904 sec\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 3000x3000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 3000x3000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 3000x3000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e5c77633e0b488e9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
